<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Dilin Wang</title>
  
  <meta name="author" content="Dilin Wang">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <link rel="shortcut icon" href="favicon.ico">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Dilin Wang</name>
              </p>
              <p>Dilin Wang is a research scientist at Meta. Prior to joining Meta, Dilin earned his PhD from the Department of Computer Science, University of Texas at Austin, where he was advised by Professor Qiang Liu. Dilin’s recent research focuses on developing algorithms that enable machines to better understand complex scenes as well as to generate content. In the meantime, he also places a strong emphasis on improving the efficiency of these algorithms for on-device settings.</p>
              <p style="text-align:center">
                <a target="_blank" href="mailto:dilinwang@utexas.edu">Email</a> &nbsp/&nbsp
                <a target="_blank" href="https://scholar.google.com/citations?user=dmTy9EIAAAAJ&hl=en">Google Scholar</a> 
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a target="_blank" href="images/dilin_wang.jpg"><img style="width:80%;max-width:80%" alt="profile photo" src="images/dilin_wang.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Selected Publications</heading>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr onmouseout="lumos_stop()" onmouseover="lumos_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                <div class="two" id='lumos_image'><img src='images/lumos.jpeg' width="110%"></div>
                <img src='images/lumos.jpeg' width="110%">
                </div>
                <script type="text/javascript">
                function lumos_start() {
                    document.getElementById('lumos_image').style.opacity = "1";
                }

                function lumos_stop() {
                    document.getElementById('lumos_image').style.opacity = "0";
                }
                lumos_stop()
                </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                            <a target="_blank" href="https://research.nvidia.com/labs/dir/lumos/">
                <papertitle>Learning to Relight Portrait Images via a Virtual Light Stage and Synthetic-to-Real Adaptation</papertitle>
                </a>
                <br>
                <strong>Yu-Ying Yeh</strong>,  
                <a target="_blank" href="https://luminohope.org/">Koki Nagano</a>, 
                <a target="_blank" href="https://www.samehkhamis.com/">Sameh Khamis</a>, 
                <a target="_blank" href="https://jankautz.com/">Jan Kautz</a>, 
                <a target="_blank" href="http://mingyuliu.net/">Ming-Yu Liu</a>,
                <a target="_blank" href="https://tcwang0509.github.io/">Ting-Chun Wang</a>
                <br>
                <em>SIGGRAPH Asia</em>, 2022  
                <br>
                <a target="_blank" href="https://research.nvidia.com/labs/dir/lumos/">project page</a> / 
                <a target="_blank" href="https://arxiv.org/abs/2209.10510">arxiv</a> / 
                <a target="_blank" href="https://youtu.be/uWSVpG0eKbU">video</a>
                <!-- <a target="_blank" href="http://imaginaire.cc/Lumos/">demo</a> -->
                <p></p>
                <p>We propose a single-image portrait relighting method trained with our rendered dataset and synthetic-to-real adaptation to achieve high photorealism without using light stage data. Our method can also handle eyeglasses and support video relighting.</p>
            </td>
          </tr>
          <tr onmouseout="photoscene_stop()" onmouseover="photoscene_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='photoscene_image'><img src='images/photoscene.gif' width="110%"></div>
                <img src='images/photoscene.jpg' width="110%">
              </div>
              <script type="text/javascript">
                function photoscene_start() {
                  document.getElementById('photoscene_image').style.opacity = "1";
                }

                function photoscene_stop() {
                  document.getElementById('photoscene_image').style.opacity = "0";
                }
                photoscene_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
							<a target="_blank" href="https://yuyingyeh.github.io/projects/photoscene.html">
                <papertitle>PhotoScene: Photorealistic Material and Lighting Transfer for Indoor Scenes</papertitle>
              </a>
              <br>
              <strong>Yu-Ying Yeh</strong>,  
              <a target="_blank" href="https://sites.google.com/a/eng.ucsd.edu/zhengqinli">Zhengqin Li</a>, 
              <a target="_blank" href="https://yannickhold.com/">Yannick Hold-Geoffroy</a>, 
              <a target="_blank" href="https://jerrypiglet.github.io/">Rui Zhu</a>, 
              <a target="_blank" href="https://cseweb.ucsd.edu/~zex014/">Zexiang Xu</a>, <br>
              <a target="_blank" href="http://www.miloshasan.net/">Miloš Hašan</a>, 
              <a target="_blank" href="http://www.kalyans.org/">Kalyan Sunkavalli</a>, 
              <a target="_blank" href="https://cseweb.ucsd.edu/~mkchandraker/">Manmohan Chandraker</a>
              <br>
              <em>CVPR</em>, 2022  
              <br>
              <a target="_blank" href="https://yuyingyeh.github.io/projects/photoscene.html">project page</a> / 
              <a target="_blank" href="http://arxiv.org/abs/2207.00757">arXiv</a> /
              <a target="_blank" href="https://openaccess.thecvf.com/content/CVPR2022/papers/Yeh_PhotoScene_Photorealistic_Material_and_Lighting_Transfer_for_Indoor_Scenes_CVPR_2022_paper.pdf">cvpr paper</a> / 
              <a target="_blank" href="https://github.com/ViLab-UCSD/PhotoScene">code</a>
              <p></p>
              <p>Transfer high-quality procedural materials and lightings from images to reconstructed indoor scene 3D geometry, which enables photorealistic 3D content creation for digital twins.</p>
            </td>
          </tr>
          
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <img src='images/openrooms_teaser.png' width="110%">
              </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <a target="_blank" href="https://vilab-ucsd.github.io/ucsd-openrooms/">
                <papertitle>OpenRooms: An Open Framework for Photorealistic Indoor Scene Datasets</papertitle>
              </a>
              <br>
              <a target="_blank" href="https://sites.google.com/a/eng.ucsd.edu/zhengqinli">Zhengqin Li</a>, 
              Ting-Wei Yu, Shen Sang, Sarah Wang, 
              <a target="_blank" href="https://sites.google.com/site/mengsong1130/">Meng Song</a>, 
              Yuhan Liu, 
              <strong>Yu-Ying Yeh</strong>, <br>
              <a target="_blank" href="https://jerrypiglet.github.io/">Rui Zhu</a>, 
              <a target="_blank" href="https://scholar.google.com/citations?user=v19p_0oAAAAJ&hl=en">Nitesh Gundavarapu</a>, 
              Jia Shi, 
              <a target="_blank" href="https://sai-bi.github.io/">Sai Bi</a>, 
              <a target="_blank" href="https://cseweb.ucsd.edu/~zex014/">Zexiang Xu</a>, 
              <a target="_blank" href="https://kovenyu.com/">Hong-Xing Yu</a>, 
              <a target="_blank" href="http://www.kalyans.org/">Kalyan Sunkavalli</a>, 
              <a target="_blank" href="http://www.miloshasan.net/">Miloš Hašan</a>, 
              <a target="_blank" href="https://cseweb.ucsd.edu/~ravir/">Ravi Ramamoorthi</a>,
              <a target="_blank" href="https://cseweb.ucsd.edu/~mkchandraker/">Manmohan Chandraker</a>
              <br>
				<em>CVPR</em>, 2021 &nbsp <font color=#d0176d><strong>(Oral Presentation)</strong></font>
              <br>
                <a target="_blank" href="https://vilab-ucsd.github.io/ucsd-openrooms/">project page</a> / 
                <a target="_blank" href="https://arxiv.org/pdf/2007.12868.pdf">arXiv</a> 	
              <p></p>
              <p>An open framework which creates a large-scale photorealistic indoor scene dataset OpenRooms from a publicly available video scans dataset ScanNet.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <img src='images/cvpr20_teaser.png' width="110%">
              </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
							<a target="_blank" href="https://cseweb.ucsd.edu//~viscomp/projects/CVPR20Transparent/">
                <papertitle>Through the Looking Glass: Neural 3D Reconstruction of Transparent Shapes</papertitle>
              </a>
              <br>
              <strong>Yu-Ying Yeh</strong><sup>*</sup>,  
              <a target="_blank" href="https://sites.google.com/a/eng.ucsd.edu/zhengqinli">Zhengqin Li</a><sup>*</sup>, 
              <a target="_blank" href="https://cseweb.ucsd.edu/~mkchandraker/">Manmohan Chandraker</a> (*equal contributions)
              <br>
				<em>CVPR</em>, 2020 &nbsp <font color=#d0176d><strong>(Oral Presentation)</strong></font>
              <br>
                <a target="_blank" href="https://cseweb.ucsd.edu//~viscomp/projects/CVPR20Transparent/">project page</a> / 
                <a target="_blank" href="https://arxiv.org/abs/2004.10904">arXiv</a> / 
                <a target="_blank" href="https://github.com/lzqsd/TransparentShapeReconstruction">code</a> / 
                <a target="_blank" href="https://github.com/lzqsd/TransparentShapeDataset">dataset</a> /
                <a target="_blank" href="https://github.com/yuyingyeh/TransparentShapeRealData">real data</a> 			
              <p></p>
              <p>Transparent shape reconstruction from multiple images captured from a mobile phone.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='images/Video_Completion.png' width="110%">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a target="_blank" href="https://ieeexplore.ieee.org/abstract/document/9099414">
                <papertitle>Static2Dynamic: Video Inference from a Deep Glimpse</papertitle>
              </a>
              <br>              
              <strong>Yu-Ying Yeh</strong>, 
              <a target="_blank" href="https://ycliu93.github.io/">Yen-Cheng Liu</a>, 
              <a target="_blank" href="https://walonchiu.github.io/">Wei-Chen Chiu</a>, 
              <a target="_blank" href="https://scholar.google.com/citations?user=HSGvdtoAAAAJ&hl=en" >Yu-Chiang Frank Wang</a>
              <br>
				<em>IEEE Transactions on Emerging Topics in Computational Intelligence</em>, 2020
              <br>
              <a target="_blank" href="https://ieeexplore.ieee.org/abstract/document/9099414">paper</a> /
              <a target="_blank" href="data/yeh2020static2dynamic.bib">bibtex</a>
              <p></p>
              <p>Video generation, interpolation, inpainting and prediction given a set of anchor frames.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='images/UFDN.png' width="110%">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a target="_blank" href="https://arxiv.org/abs/1809.01361">
                <papertitle>A Unified Feature Disentangler for Multi-Domain Image Translation and Manipulation</papertitle>
              </a>
              <br>              
              <a target="_blank" href="https://alexander-h-liu.github.io/">Alexendar Liu</a>, 
              <a target="_blank" href="https://ycliu93.github.io/">Yen-Cheng Liu</a>, 
              <strong>Yu-Ying Yeh</strong>, 
              <a target="_blank" href="https://scholar.google.com/citations?user=HSGvdtoAAAAJ&hl=en" >Yu-Chiang Frank Wang</a>
              <br>
				<em>NeurIPS</em>, 2018
              <br>
              <a target="_blank" href="https://arxiv.org/abs/1809.01361">arXiv</a> /
              <a target="_blank" href="data/liu2018unified.bib">bibtex</a> /
              <a target="_blank" href="https://github.com/Alexander-H-Liu/UFDN">code</a>
              <p></p>
              <p>A novel and unified deep learning framework which is capable of learning domain-invariant representation from data across multiple domains.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='images/CDRD.png' width="110%">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a target="_blank" href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Liu_Detach_and_Adapt_CVPR_2018_paper.pdf">
                <papertitle>Detach and Adapt: Learning Cross-Domain Disentangled Deep Representation</papertitle>
              </a>
              <br>
              <a target="_blank" href="https://ycliu93.github.io/">Yen-Cheng Liu</a>, 
              <strong>Yu-Ying Yeh</strong>, 
              Tzu-Chien Fu, 
              <a target="_blank" href="https://walonchiu.github.io/">Wei-Chen Chiu</a>, <br>
              Sheng-De Wang, 
              <a target="_blank" href="https://scholar.google.com/citations?user=HSGvdtoAAAAJ&hl=en" >Yu-Chiang Frank Wang</a>
              <br>
				<em>CVPR</em>, 2018 &nbsp <font color=#d0176d><strong>(Spotlight Presentation)</strong></font>
              <br>
              <a target="_blank" href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Liu_Detach_and_Adapt_CVPR_2018_paper.pdf">paper</a> /
              <a target="_blank" href="data/liu2018detach.bib">bibtex</a> /
              <a target="_blank" href="https://github.com/ycliu93/CDRD">code</a> /
              <a target="_blank" href="https://youtu.be/sIkUzmgUaxc?t=4148">presentation</a>
              <p></p>
              <p>Feature disentanglement for cross-domain data which enables image translation and manipulation from labeled source doamin to unlabeled target domain.</p>
            </td>
          </tr>

        </tbody></table>


      </td>
    </tr>
  </table>
</body>

</html>
